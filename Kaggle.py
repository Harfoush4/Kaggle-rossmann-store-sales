# -*- coding: utf-8 -*-
"""Copy_ABdo of Kaggle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQUilrgNoviihoA-Haknnxhgk2OTFQhw
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

#Importing the dataset
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
store_df = pd.read_csv('store.csv')
train_df.head()

#Dropping out Id coloumn to make it smoother
test_df.drop(['Id'],axis=1,inplace=True)
train_df.drop(["Customers"],axis=1,inplace=True)

# use the checking method as a function

# Checking for missing values in the training dataset
missing_values = train_df.isnull().sum()
missing_percentage = (missing_values / len(train_df)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info.head())

# Check for missing values in the test dataset
missing_values = test_df.isnull().sum()
missing_percentage = (missing_values / len(test_df)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info.head())

#dropping NA since it's 0.026%
test_df.fillna(0, inplace=True)
missing_values = test_df.isnull().sum()
missing_percentage = (missing_values / len(test_df)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info.head())

train_store = pd.merge(train_df, store_df, on='Store')
test_store = pd.merge(test_df, store_df, on='Store')
train_store.head()

missing_values = train_store.isnull().sum()
missing_percentage = (missing_values / len(train_store)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info.head())

missing_values = test_store.isnull().sum()
missing_percentage = (missing_values / len(test_store)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info)

train_store['CompetitionDistance'].fillna(train_store['CompetitionDistance'].median(), inplace=True)
test_store['CompetitionDistance'].fillna(test_store['CompetitionDistance'].median(), inplace=True)

train_store['PromoInterval'].fillna(0, inplace=True)
test_store['PromoInterval'].fillna(0, inplace=True)

train_store['CompetitionOpenSinceYear'].fillna(0, inplace=True)
train_store['CompetitionOpenSinceMonth'].fillna(0, inplace=True)
test_store['CompetitionOpenSinceYear'].fillna(0, inplace=True)
test_store['CompetitionOpenSinceMonth'].fillna(0, inplace=True)

train_store['Promo2SinceYear'].fillna(0, inplace=True)
train_store['Promo2SinceWeek'].fillna(0, inplace=True)
test_store['Promo2SinceYear'].fillna(0, inplace=True)
test_store['Promo2SinceWeek'].fillna(0, inplace=True)

missing_values = test_store.isnull().sum()
missing_percentage = (missing_values / len(test_store)) * 100
missing_data_info = pd.DataFrame({'Missing_Values': missing_values, 'Missing_Percentage': missing_percentage})
missing_data_info = missing_data_info.sort_values(by='Missing_Percentage', ascending=False)
print("Columns with the highest percentage of missing values:")
print(missing_data_info)

train_store

print("Closed stores with sales are ",((train_store["Open"]==0) & (train_store["Sales"]>0)).sum())
print("Open stores with zero sales are",((train_store["Open"]==1) & (train_store["Sales"]<=0)).sum())
print("negative sales are ",(train_store["Sales"]<0).sum())

train_store=train_store[~((train_store["Open"]==1) & (train_store["Sales"]<=0))]
print("Open stores with zero sales are",((train_store["Open"]==1) & (train_store["Sales"]<=0)).sum())

from scipy.stats import zscore
numeric = train_store.select_dtypes(include=['number'])


z_scores = np.abs(zscore(numeric))
outliers = (z_scores > 3)
print(f"Number of rows before removing outliers: {train_store.shape[0]}")
train_store = train_store[~np.any(outliers, axis=1)]
print(f"Number of rows after removing outliers: {train_store.shape[0]}")

train_store['Date'] = pd.to_datetime(train_df['Date'])
test_store['Date'] = pd.to_datetime(test_df['Date'])

train_store['Week'] = pd.to_datetime(train_store['Date']).dt.isocalendar().week
train_store['DayOfYear'] = pd.to_datetime(train_store['Date']).dt.dayofyear
train_store['DayOfMonth'] = pd.to_datetime(train_store['Date']).dt.day
train_store['Year'] = pd.DatetimeIndex(train_store['Date']).year
train_store['Quarter'] = pd.DatetimeIndex(train_store['Date']).quarter
train_store['Month'] = pd.DatetimeIndex(train_store['Date']).month
#train_store.sort_values(by=['Date'],ascending=True,inplace=True)


test_store['Week'] = pd.to_datetime(test_store['Date']).dt.isocalendar().week
test_store['DayOfYear'] = pd.to_datetime(test_store['Date']).dt.dayofyear
test_store['DayOfMonth'] = pd.to_datetime(test_store['Date']).dt.day
test_store['Year'] = pd.DatetimeIndex(test_store['Date']).year
test_store['Quarter'] = pd.DatetimeIndex(test_store['Date']).quarter
test_store['Month'] = pd.DatetimeIndex(test_store['Date']).month
#test_store.sort_values(by=['Date'],ascending=True,inplace=True)


train_store.drop(['Date'],axis=1,inplace=True)
test_store.drop(['Date'],axis=1,inplace=True)
train_store.head()
#test_store.head()

#Manually encoding the stateholiday coloumn before using pre made encoders:
train_store['StateHoliday'].replace("a", 1, inplace=True)
train_store['StateHoliday'].replace("b", 2, inplace=True)
train_store['StateHoliday'].replace("c", 3, inplace=True)
train_store['StateHoliday'].replace('0', 0, inplace=True)
train_store['StateHoliday'].unique()
test_store['StateHoliday'].replace("a", 1, inplace=True)
test_store['StateHoliday'].replace("b", 2, inplace=True)
test_store['StateHoliday'].replace("c", 3, inplace=True)
test_store['StateHoliday'].replace('0', 0, inplace=True)
test_store['StateHoliday'].unique()

from sklearn.preprocessing import LabelEncoder
#try one hot encoder maybe better
category_train_store = train_store.select_dtypes(exclude=[np.number]).columns.tolist()
print("Categorical features in train dataset:", category_train_store)

Categorical_test_store = test_store.select_dtypes(exclude=[np.number]).columns.tolist()
print("Categorical features in test dataset:", Categorical_test_store)

le = LabelEncoder()
for column in category_train_store:
    train_store[column] = le.fit_transform(train_store[column].astype(str))

for column in Categorical_test_store:
    test_store[column] = le.fit_transform(test_store[column].astype(str))

print("Encoded features in train dataset:\n", train_store[category_train_store].head())
print("Encoded features in test dataset:\n", test_store[Categorical_test_store].head())

scaler = MinMaxScaler()
train_scaling=train_store.drop(["Sales"],axis=1,inplace=False)
scaled_x_train_store = pd.DataFrame(scaler.fit_transform(train_scaling),columns=train_scaling.columns)
scaled_test_store = scaler.fit_transform(test_store)
scaled_y_train_store = pd.DataFrame(scaler.fit_transform(pd.DataFrame(train_store["Sales"])),columns=["Sales"])
scaled_train_store=pd.concat([scaled_x_train_store, scaled_y_train_store], axis=1)
scaled_test_store = pd.DataFrame(scaled_test_store, columns=test_store.columns)

print(scaled_train_store.head())

print(scaled_test_store.head())
print(scaled_train_store.head())

#using the same error calc eqn provided by the compiti
def rmspe(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2))

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
import matplotlib.pyplot as plt

x=scaled_train_store.drop(['Sales'],axis=1)
y=scaled_train_store['Sales']
print(y)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
# print(X_train.head())
# print(X_test.head())
# print(y_train.head())
# print(y_test.head())

from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
model = models.Sequential([
    layers.Input(shape=(x_train.shape[1],)),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),  #s
    layers.Dense(64, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)

])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1)
]
# Compile model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
model.summary()

# Train the model
batch_size = 32
history = model.fit(x_train, y_train, validation_data=(x_test, y_test),
                    epochs=10, callbacks=callbacks, batch_size=batch_size)

# Plot training history
plt.plot(history.history['loss'])


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

print(x_test.head())
print(scaled_test_store.head())

# Evaluate the model
test_loss, test_mae = model.evaluate(x_test, y_test)
print(f"Test Mean Absolute Error: {test_mae}")

# Predict sales for a new dataset
predicted_sales = model.predict(scaled_test_store)  # Ensure new_data is normalized the same way as x_train
predicted_sales=scaler.inverse_transform(predicted_sales)
print("Predicted Sales:", predicted_sales)

print(type(predicted_sales))

predicted_sales = predicted_sales.flatten()
df = pd.DataFrame({
    "ID": range(1, len(predicted_sales) + 1),
    "Sales": predicted_sales
})
print(df)

df.to_csv('submission.csv', index=False)